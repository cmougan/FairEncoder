{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae22df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#from pandas_profiling import ProfileReportofileReport\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1512bb96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import selection_rate, false_positive_rate,true_positive_rate,count\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression,Lasso\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBRegressor,XGBClassifier\n",
    "import shap\n",
    "\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.m_estimate import MEstimateEncoder\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1cfe99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pd.read_csv('propublica_data_for_fairml.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83bdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/compas-scores-raw.csv\")\n",
    "\n",
    "df[\"Score\"] = df[\"DecileScore\"]\n",
    "\n",
    "# df.loc[df[\"DecileScore\"] > 7, \"Score\"] = 2\n",
    "# df.loc[(df[\"DecileScore\"] > 4) & (df[\"DecileScore\"] < 8), \"Score\"] = 1\n",
    "# df.loc[df[\"DecileScore\"] < 5, \"Score\"] = 0\n",
    "\n",
    "df.loc[df[\"DecileScore\"] > 4, \"Score\"] = 1\n",
    "df.loc[df[\"DecileScore\"] <= 4, \"Score\"] = 0\n",
    "\n",
    "\n",
    "cols = [\n",
    "    \"Person_ID\",\n",
    "    \"AssessmentID\",\n",
    "    \"Case_ID\",\n",
    "    \"LastName\",\n",
    "    \"FirstName\",\n",
    "    \"MiddleName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"ScaleSet_ID\",\n",
    "    \"Screening_Date\",\n",
    "    \"RecSupervisionLevel\",\n",
    "    \"Agency_Text\",\n",
    "    \"AssessmentReason\",\n",
    "    \"Language\",\n",
    "    \"Scale_ID\",\n",
    "    \"IsCompleted\",\n",
    "    \"IsDeleted\",\n",
    "    \"AssessmentType\",\n",
    "    \"DecileScore\",\n",
    "]\n",
    "\n",
    "\n",
    "df = df.drop(columns=cols)\n",
    "\n",
    "possible_targets = [\"RawScore\", \"ScoreText\", \"Score\"]\n",
    "\n",
    "X = df.drop(columns=possible_targets)\n",
    "y = df[[\"Score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4436fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b0bffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:25:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "te = TargetEncoder()\n",
    "model = XGBClassifier()\n",
    "pipe = Pipeline([('encoder', te), ('model', model)])\n",
    "\n",
    "pipe.fit(X_tr,y_tr)\n",
    "\n",
    "preds = pipe.predict(X_te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(pipe.named_steps[\"model\"])\n",
    "shap_values = explainer.shap_values(pipe[:-1].transform(X_tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abdc333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc623bc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gm = MetricFrame(\n",
    "    metrics=accuracy_score,\n",
    "    y_true=y_te,\n",
    "    y_pred=preds,\n",
    "    sensitive_features=X_te[\"Sex_Code_Text\"],\n",
    ")\n",
    "print(gm.overall)\n",
    "print(gm.by_group)\n",
    "\n",
    "gm = MetricFrame(\n",
    "    metrics=accuracy_score,\n",
    "    y_true=y_te,\n",
    "    y_pred=preds,\n",
    "    sensitive_features=X_te[\"Ethnic_Code_Text\"],\n",
    ")\n",
    "\n",
    "print(gm.by_group)\n",
    "\n",
    "gm = MetricFrame(\n",
    "    metrics=accuracy_score,\n",
    "    y_true=y_te,\n",
    "    y_pred=preds,\n",
    "    sensitive_features=X_te[\"RecSupervisionLevelText\"],\n",
    ")\n",
    "\n",
    "print(gm.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(modelo, enc, data, target, test):\n",
    "    pipe = Pipeline([(\"encoder\", enc), (\"model\", modelo)])\n",
    "    pipe.fit(data, target)\n",
    "    return pipe.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59264956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec17b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for metrics in [selection_rate, false_positive_rate, true_positive_rate]:\n",
    "    gms = []\n",
    "    gms_rec = []\n",
    "    ms = []\n",
    "\n",
    "    param = [0, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10_000, 20_000]\n",
    "    # np.linspace(0,X_tr.shape[0],20)\n",
    "    for m in tqdm(param):\n",
    "        encoder = MEstimateEncoder(m=m)\n",
    "        model = LogisticRegression()\n",
    "        # model = GradientBoostingClassifier()\n",
    "\n",
    "        preds = fit_predict(\n",
    "            modelo=model, enc=encoder, data=X_tr, target=y_tr, test=X_te\n",
    "        )\n",
    "        gm = MetricFrame(\n",
    "            metrics=metrics,\n",
    "            y_true=y_te,\n",
    "            y_pred=preds,\n",
    "            sensitive_features=X_te[\"Ethnic_Code_Text\"],\n",
    "        )\n",
    "        gm_rec = MetricFrame(\n",
    "            metrics=metrics,\n",
    "            y_true=y_te,\n",
    "            y_pred=preds,\n",
    "            sensitive_features=X_te[\"RecSupervisionLevelText\"],\n",
    "        )\n",
    "\n",
    "        gms.append(gm)\n",
    "        gms_rec.append(gm_rec)\n",
    "        ms.append(m)\n",
    "\n",
    "    plt.figure()\n",
    "    title = \"Impact of encoding regularization in category fairness \" + str(\n",
    "        metrics.__name__\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"M parameter\")\n",
    "    plt.plot(ms, [gm.overall for gm in gms_rec], label=\"Overall\")\n",
    "    plt.plot(ms, [gm.by_group[\"Low\"] for gm in gms_rec], label=\"Low\")\n",
    "    plt.plot(ms, [gm.by_group[\"High\"] for gm in gms_rec], label=\"High\")\n",
    "    plt.plot(ms, [gm.by_group[\"Medium\"] for gm in gms_rec], label=\"Medium\")\n",
    "    plt.plot(\n",
    "        ms,\n",
    "        [gm.by_group[\"Medium with Override Consideration\"] for gm in gms_rec],\n",
    "        label=\"Medium with Override Consideration\",\n",
    "    )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    title = \"Impact of encoding regularization in category fairness \" + str(\n",
    "        metrics.__name__\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"M parameter\")\n",
    "    plt.plot(ms, [gm.overall for gm in gms], label=\"Overall\")\n",
    "    plt.plot(ms, [gm.by_group[\"Caucasian\"] for gm in gms], label=\"Caucasian\")\n",
    "    plt.plot(\n",
    "        ms, [gm.by_group[\"African-American\"] for gm in gms], label=\"AfricanAmerican\"\n",
    "    )\n",
    "    plt.plot(ms, [gm.by_group[\"Arabic\"] for gm in gms], label=\"Arabic\")\n",
    "    plt.plot(ms, [gm.by_group[\"Hispanic\"] for gm in gms], label=\"Hispanic\")\n",
    "    # plt.plot(ms,[gm.by_group['Oriental'] for gm in gms],label='Oriental')\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190705f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8efee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694bb07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80314ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d33a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ba0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad464d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7448c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05269bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f46c319",
   "metadata": {},
   "source": [
    "# Other Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "X = pd.get_dummies(data.data)\n",
    "y_true = (data.target == '>50K') * 1\n",
    "sex = data.data['sex']\n",
    "sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c871d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=4)\n",
    "classifier.fit(X, y_true)\n",
    "DecisionTreeClassifier()\n",
    "y_pred = classifier.predict(X)\n",
    "gm = MetricFrame(metrics=accuracy_score, y_true=y_true, y_pred=y_pred, sensitive_features=sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7699d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5099919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a25ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f6189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7524a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf0e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a94def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad3819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "9fe83bef",
   "metadata": {},
   "source": [
    "gms = []\n",
    "gms_rec = []\n",
    "ms = []\n",
    "\n",
    "param = [0, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10_000, 20_000]\n",
    "# np.linspace(0,X_tr.shape[0],20)\n",
    "for m in tqdm(param):\n",
    "    encoder = MEstimateEncoder(m=m)\n",
    "    model = LogisticRegression()\n",
    "    # model = GradientBoostingClassifier()\n",
    "\n",
    "    preds = fit_predict(modelo=model, enc=encoder, data=X_tr, target=y_tr, test=X_te)\n",
    "    gm = MetricFrame(\n",
    "        metrics=selection_rate,\n",
    "        y_true=y_te,\n",
    "        y_pred=preds,\n",
    "        sensitive_features=X_te[\"Ethnic_Code_Text\"],\n",
    "    )\n",
    "    gm_rec = MetricFrame(\n",
    "        metrics=selection_rate,\n",
    "        y_true=y_te,\n",
    "        y_pred=preds,\n",
    "        sensitive_features=X_te[\"RecSupervisionLevelText\"],\n",
    "    )\n",
    "\n",
    "    gms.append(gm)\n",
    "    gms_rec.append(gm_rec)\n",
    "    ms.append(m)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Impact of encoding regularization in category fairness ')\n",
    "plt.xlabel('M parameter')\n",
    "plt.plot(ms,[gm.overall for gm in gms_rec],label='Overall')\n",
    "plt.plot(ms,[gm.by_group['Low'] for gm in gms_rec],label='Low')\n",
    "plt.plot(ms,[gm.by_group['High'] for gm in gms_rec],label='High')\n",
    "plt.plot(ms,[gm.by_group['Medium'] for gm in gms_rec],label='Medium')\n",
    "plt.plot(ms,[gm.by_group['Medium with Override Consideration'] for gm in gms_rec],label='Medium with Override Consideration')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.1, 1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantile",
   "language": "python",
   "name": "quantile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
